

# 1. 概率论

## 1.1. 随机变量

随机变量（random variable）是可以随机地取不同值的变量。

## 1.2. 概率分布

概率分布（probability distribution）用来描述随机变量或一簇随机变量在每一
个可能取到的状态的可能性大小。

### 1.2.1. 离散型变量和概率质量函数

**概率质量函数（probability mass function, PMF）** 将随机变量能够取得的每个状态映射到随机变量取得该状态的概率。

概率质量函数可以同时作用于多个随机变量。这种多个变量的概率分布被称为 **联合概率分布（joint probability distribution）** 。

### 1.2.2. 连续型变量和概率密度函数

**概率密度函数（probability density function, PDF）** $p(x)$ 并没有直接对特定的状态给出概率，相对的，它给出了落在面积为 $\delta x$ 的无限小的区域内的概率为 $p(x)\delta x$。

## 1.3. 边缘概率

知道了一组变量的联合概率分布，但想要了解其中一个子集的概率分布。这种定义在子集上的概率分布被称为 **边缘概率分布（marginal probability distribution）**。

离散型随机变量的边缘概率使用求和法则：

$$
\forall{x}\in{\mathrm{x}}, P(\mathrm{x}= x) = \sum_yP(\mathrm{x}=x, \mathrm{y}=y)
$$

对于连续型变量，我们需要用积分替代求和：

$$
p(x)=\int{p(x,y)dy}
$$

## 1.4. 条件概率

条件概率可以通过下面的公式计算：

$$
P(\mathrm{y}=y \mid \mathrm{x}=x) = \frac{P(\mathrm{y}=y, \mathrm{x}=x)}{P(\mathrm{x}=x)}
$$

## 1.5. 条件概率的链式法则

任何多维随机变量的联合概率分布,都可以分解成只有一个变量的条件概率相乘的形式:

$$
P(x^{(1)},\ldots,x^{(n)})= P(x^{(1)})\prod_{i=2}^nP(x^{(i)} \mid x^{(1)},\ldots,x^{(i-1)})
$$


## 1.6. 独立性和条件独立性

两个随机变量 x 和 y,如果它们的概率分布可以表示成两个因子的乘积形式,并且一个因子只包含 x 另一个因子只包含 y,我们就称这两个随机变量是 **相互独立的(independent):** $x\bot{y}$

$$
\forall{x}\in{\mathrm{x}},y\in{\mathrm{y}},p(\mathrm{x}=x,\mathrm{y}=y)=p(\mathrm{x}=x)p(\mathrm{y}=y)
$$

如果关于 x 和 y 的条件概率分布对于 z 的每一个值都可以写成乘积的形式,那么这两个随机变量 x 和 y 在给定随机变量 z 时是 **条件独立的(conditionally independent):** $x\bot{y}\mid{z}$

$$
\forall{x}\in{\mathrm{x}},y\in{\mathrm{y}},z\in{\mathrm{z}},p(\mathrm{x}=x, \mathrm{y}=y \mid \mathrm{z}=z)=p(\mathrm{x}=x\mid{\mathrm{z}=z})p(\mathrm{y}=y\mid{\mathrm{z}=z})
$$

## 1.7. 期望、方差和协方差

函 数 $f(x)$ 关 于 某 分 布 $P(\rm{x})$ 的 **期 望(expectation)** 或 者 **期 望 值(expected value)** 是指,当 $x$ 由 $P$ 产生, $f$ 作用于 $x$ 时,$f(x)$ 的平均值。

离散型随即变量通过求和得到：

$$
\Bbb{E}_{x\sim{P}}[f(x)]=\sum_x{P}(x)f(x)
$$

连续型随机变量可以通过求积分得到:

$$
\Bbb{E}_{x\sim{P}}[f(x)]=\int{p(x)f(x)dx}
$$

**方差(variance)** 衡量的是当我们对 $x$ 依据它的概率分布进行采样时,随机变量 $\rm{x}$ 的函数值会呈现多大的差异:

$$
Var(f(x))=\Bbb{E}[(f(x)-\Bbb{E}[f(x)])^2]
$$

**协方差(covariance)** 在某种意义上给出了两个变量线性相关性的强度以及这些变量的尺度:

$$
Cov(f(x),g(y))=\Bbb{E}[(f(x)-\Bbb{E}[f(x)])(g(y)-\Bbb{E}[g(y)])]
$$

两个变量如果协方差为零,它们之间一定没有线性关系。独立性比零协方差的要求更强,因为独立性还排除了非线性的关系。

随机向量 $x\in{R^n}$ 的 **协方差矩阵(covariance matrix)** 是一个 $n\times{n}$ 的矩阵,并且满足:

$$
Cov(\rm{x})_{i,j} = Cov(\rm{x}_i,\rm{x}_j)
$$

协方差矩阵的对角元是方差:

$$
Cov(\rm{x}_i,\rm{x}_i) = Var(\rm{x}_i)
$$

## 1.8. 常用概率分布

### 1.8.1. Bernoulli 分布

$$
\begin{gathered}
P(\rm{x}=1) = \phi \\
P(\rm{x}=0) = 1 - \phi \\
P(\rm{x}=\it{x}) = \phi{^x}(1-\phi)^{1-x} \\
\Bbb{E}_{\rm{x}}[\rm{x}] = \phi \\
Var_{\rm{x}}(\rm{x}) = \phi(1-\phi)
\end{gathered}
$$

### 1.8.2. Multinoulli 分布

**Multinoulli 分布(multinoulli distribution)** 或者 **范畴分布(categorical distribution)** 是指在具有 $k$ 个不同状态的单个离散型随机变量上的分布,其中 $k$ 是一个有限值。

### 1.8.3. 高斯分布

实数上最常用的分布就是 **正态分布(normal distribution)**,也称为 **高斯分布(Gaussian distribution):**

$$
\mathcal{N}(x;\mu,\sigma{^2}) =\sqrt{\frac{1}{2\pi\sigma{^2}}}\exp(-\frac{1}{2\sigma{^2}}(x-\mu)^2)
$$

> 分布的均值: $\Bbb{E}[\rm{x}] = \mu$,分布的标准差用 $\sigma$ 表示,方差用 $\sigma{^2}$ 表示。

### 1.8.4. 指数分布和 Laplace 分布

我们经常会需要一个在 $x = 0$ 点处取得边界点 (sharp point) 的分布。为了实现这一目的,我们可以使用 **指数分布(exponential distribution):**

$$
p(x;\lambda)=\lambda\bold{1}_{x\geq{0}}\exp(-\lambda{x})
$$

一个联系紧密的概率分布是 **Laplace 分布(Laplace distribution)**,它允许我们在任意一点 $\mu$ 处设置概率质量的峰值

$$
Laplace(x;\mu,\gamma)=\frac{1}{2\gamma}\exp\left(-\frac{|x-\mu|}{\gamma}\right)
$$

### 1.8.5. Dirac 分布和经验分布

我们希望概率分布中的所有质量都集中在一个点上。这可以通过 Dirac delta 函数(Dirac delta function)δ(x) 定义概率密度函数来实现:

$$
p(x)=\delta(x-\mu)
$$

$\delta(x)$ 是一种不同类型的数学对象,被称为 **广义函数(generalized function)**,广义函数是依据积分性质定义的数学对象。

Dirac 分布经常作为 **经验分布(empirical distribution)** 的一个组成部分出现:

$$
\hat{p}(\bold{x})=\frac{1}{m}\sum_{i=1}^{m}\delta(\bold{x}-\bold{x}^{(i)})
$$

## 1.9. 常用函数的有用性质

- logistic sigmoid 函数:

$$
\sigma(x)=\frac{1}{1+\exp(-x)}
$$

> sigmoid 函数在变量取绝对值非常大的正值或负值时会出现 **饱和(saturate)** 现象,意味着函数会变得很平,并且对输入的微小改变会变得不敏感。

- softplus 函数(softplus function)

$$
\zeta(x)=\log(1+\exp(x))
$$

需要记住的性质：

$$
\begin{gathered}
\sigma(x) = \frac{\exp(x)}{\exp(x)+\exp(0)} \\
\frac{d}{dx}\sigma(x)=\sigma(x)(1-\sigma(x)) \\
1 - \sigma(x)=\sigma(-x) \\
\log{\sigma(x)} = -\zeta(-x) \\
\frac{d}{dx}\zeta(x)=\sigma(x) \\
\forall{x}\in(0,1),\sigma^{-1}(x)=\log\left(\frac{x}{1-x}\right) \\
\forall{x}\gt0,\zeta^{-1}(x)=\log(\exp(x)-1) \\
\zeta(x)=\int_{-\infty}^{x}\sigma(y)dy \\
\zeta(x) - \zeta(-x)=x
\end{gathered}
$$

## 1.10. 贝叶斯规则

已知 $P(\rm{y}\mid{\rm{x}})$ 时计算 $P(\rm{x}\mid{\rm{y}})$,我们可以用 **贝叶斯规则(Bayes’ rule)** 来实现这一目的:

$$
P(\rm{x}\mid{\rm{y}})=\frac{P(\rm{x})P(\rm{y}\mid{\rm{x}})}{P(\rm{y})}
$$

其中 $P(\rm{y})=\sum_xP(\rm{y}\mid{\it{x}})P(\it{x})$

## 信息论

定义一个事件 $\rm{x} = \it{x}$ 的 **自信息(self-information)** 为

$$
I(x) = -\log{P(x)}
$$

**香农熵(Shannon entropy)** 来对整个概率分布中的不确定性总量进行量化:

$$
H(\mathrm{x}) = \Bbb{E}_{x\sim{P}}[I(x)] = -\Bbb{E}_{x\sim{P}}[\log{P(x)}]
$$

> 一个分布的香农熵是指遵循这个分布的事件所产生的期望信息总量。

同一个随机变量 $\mathrm{x}$ 有两个单独的概率分布 $P(\mathrm{x})$ 和 $Q(\mathrm{x})$,我们可以使用 **KL 散度(Kullback-Leibler (KL) divergence)** 来衡量这两个分布的差异:

$$
D_{KL}(P||Q) = \Bbb{E}_{x\sim{P}}\left[\log\frac{P(x)}{Q(x)}\right] = \Bbb{E}_{x\sim{P}}[\log{P(x)}-\log{Q(x)}]
$$

一个和 KL 散度密切联系的量是 **交叉熵(cross-entropy)**: $H(P,Q)=H(P)+D_{KL}(P||Q)$

$$
H(P,Q)=-\Bbb{E}_{x\sim{P}}\log{Q(x)}
$$

## 结构化概率模型

用图来表示这种概率分布的分解,我们把它称为 **结构化概率模型(structured probabilistic model)** 或者 **图模型 (graphical model)** ,其中图的每个节点对应着一个随机变量,连接两个随机变量的边意味着概率分布可以表 示成这两个随机变量之间的直接作用。

- **有向(directed)** 模型使用带有有向边的图,它们用条件概率分布来表示分解,有向模型对于分布中的每一个随机变量 $\mathrm{x}_i$ 都包含着一个影响因子,这个组成 $\mathrm{x}_i$ 条件概率的影响因子被称为 $\mathrm{x}_i$ 的父节点,记为 $Pa_\mathcal{G}(\mathrm{x}_i )$:

$$
p(\mathrm{x})=\prod_ip(\mathrm{x}_i\mid{Pa_\mathcal{G}(\mathrm{x}_i)})
$$

- 
