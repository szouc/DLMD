Reforcement Learning

# 1. Basic Concepts

## 1.1. reward

* A scalar feedback signal.

## 1.2. state

* The information (determine what happens next)

## 1.3. return

* The total discounted reward from time-step t

$$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... = \sum_{k=0}^\infty \gamma^kR_{t+k+1}$$

## 1.4. value function

* A prediction of future reward.
* The expected return starting from state $s$.

$$
v(s) = \mathbb{E}[G_t | S_t = s] =\mathbb{E}[R_{t+1} + \gamma v(S_{t+1})| S_t = s] = R_s + \gamma\sum_{s'\in{S}}P_{ss'}v(s')
$$

## 1.5. policy

* A map from state to action.
* A distribution over actions given state $s$.

$$
\pi(a|s) = \mathbb{P}[A_t = a | S_t = s]
$$

## 1.6. action-value function

* The expected return starting from state $s$, taking action $a$, following policy $\pi$.

$$
q_{\pi}(s,a) = \mathbb{E}[G_t | S_t = s, A_t = a]
$$

# 2. Bellman Expectation Equation

$$
v_{\pi}(s) = \sum_{a\in{A}}\pi(a|s)(R_s^a + \gamma\sum_{s'\in{S}}P_{ss'}^av_{\pi}(s')) = \sum_{a\in{A}}\pi(a|s)q_{\pi}(s,a)
$$

# 3. Dynamic Programming

D.P. assumes full knowledge of the M.D.P.

## 3.1. Prediction

* Iterative Policy Evaluation (B.E.E.)

$$
\boldsymbol{v}^{k+1} = \boldsymbol{R}^{\pi} + \gamma\boldsymbol{P}^{\pi}\boldsymbol{v}^k
$$

## 3.2. Control

* Policy Iteration (B.E.E + Greedy Policy Improvement)

$$
\pi'(s) = \arg\max_{a\in{A}}q_{\pi}(s,a)
$$

* Value Iteration (B.O.E.)

$$
\boldsymbol{v}_{k+1} = \max_{a\in{A}}\boldsymbol{R}^a + \gamma\boldsymbol{P}^a\boldsymbol{v}_k
$$

# 4. Model-Free Prediction

## 4.1. Monte-Carlo

* M.C. learns from complete episodes.
* M.C. policy evaluation uses empirical mean return instead of expected return.

$$
v(s_t) \gets v(s_t) + \alpha(G_t - v(s_t))
$$

## 4.2. Temporal-Difference

* T.D. learns from incomplete episodes.
* T.D. policy evaluation uses estimated return instead of actual return.

$$
v(s_t) \gets v(s_t) + \alpha(R_{t+1} + \gamma v(s_{t+1}) - v(s_t))
$$

## 4.3. T.D.($\lambda$)

### 4.3.1. Forward-view

* using weight $(1-\lambda)\lambda^{n-1}$

$$
G_t^\lambda = (1-\lambda)\lambda^{n-1}G_t^{(n)}
$$

* Only be computed from complete episodes.

$$
v(s_t) \gets v(s_t) + \alpha(G_t^\lambda - v(s_t))
$$

### 4.3.2. Backward-view

* Keep an eligibility trace for every state $s$.

$$
E_0(s) = 0
$$

$$
E_t(s) = \gamma\lambda{E_{t-1}(s)} + \boldsymbol{1}(s_t = s)
$$

* Update value $v(s)$ for every state $s$, from imcomplete episodes.

$$
\delta_t=R_{t+1} + \gamma{v(s_{t+1})} - v(s_t)
$$

$$
v(s) \gets v(s) + \alpha\delta_t{E_t(s)}
$$

# 5. Model-Free Control

## 5.1. On-policy

* Learn about policy $\pi$ from experience sampled from $\pi$.

### 5.1.1. GLIE M.C. control

* For each state $S_t$ and action $A_t$ in the $k$th episode.

$$
N(S_t, A_t) \gets N(S_t, A_t) + 1
$$

$$
Q(S_t, A_t) \gets Q(S_t, A_t) + \frac{1}{N(S_t,A_t)}(G_t -Q(S_t,A_t))
$$

* Improve policy based on new action-value function.

$$
\epsilon \gets \frac{1}{k}
$$

$$
\pi \gets \epsilon-greedy(Q)
$$

### 5.1.2. Sarsa

* Updating every time-step

$$
Q(S,A) \gets Q(S,A) + \alpha(R + \gamma{Q(S',A') - Q(S,A)})
$$

### 5.1.3. Sarsa($\lambda$)

* Forward-view

$$
Q(S_t,A_t) \gets Q(S_t,A_t) + \alpha(q_t^\lambda - Q(S_t,A_t))
$$

* Backward-view

$$
Q(s,a) \gets Q(s,a) + \alpha\delta_t{E_t(s,a)}
$$

## 5.2. Off-policy

Learn about policy $\pi$ from experience sampled from $\mu$.

### 5.2.1. Off-policy T.D. control

* Use T.D. target generated from $\mu$ to evaluate $\pi$.

$$
v(s_t) \gets v(s_t) + \alpha(\frac{\pi(A_t|S_t)}{\mu(A_t|S_t)}(R_{t+1} + \gamma{v(s_{t+1})}) - v(s_t))
$$

### 5.2.2. Q-Learning

* Next action is chosen using behavior policy $A_{t+1} \thicksim \mu(\cdot|S_t)$.
* Consider alternative successor action $A' \thicksim \pi(\cdot|S-t)$.
* The target policy $\pi$ is greedy.
* The behavior policy $\mu$ is $\epsilon$-greedy.

$$
Q(s,a) \gets Q(s,a) + \alpha(R + \gamma{\max_{a'}}Q(s',a') - Q(s,a))
$$